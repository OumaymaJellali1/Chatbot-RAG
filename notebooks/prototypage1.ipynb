{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9c7c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import psycopg\n",
    "from pgvector import Vector\n",
    "from pgvector.psycopg import register_vector\n",
    "from transformers import pipeline, AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "DATA_DIR = \"data/TRANS_TXT\" \n",
    "db_params = {\n",
    "    'dbname': 'rag_chatbot',\n",
    "    'user': 'postgres',\n",
    "    'password': '11649303',\n",
    "    'host': 'localhost',\n",
    "    'port': '5432'\n",
    "}\n",
    "VECTOR_DIM = 384  \n",
    "\n",
    "\n",
    "print(\"Chargement du modèle d'embedding...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "embedding_model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "print(\"Modèle d'embedding chargé !\")\n",
    "\n",
    "print(\"Chargement du modèle de génération...\")\n",
    "generator = pipeline('text2text-generation', model='google/flan-t5-base')\n",
    "print(\"Modèle de génération chargé !\")\n",
    "\n",
    "\n",
    "\n",
    "def embed_text(text: str):\n",
    "    \"\"\"Calcule l'embedding d'un texte via Hugging Face et renvoie une liste\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        embeddings = embedding_model(**inputs).last_hidden_state.mean(dim=1)\n",
    "    return embeddings.squeeze().tolist()\n",
    "\n",
    "\n",
    "def create_corpus_list(data_dir: str):\n",
    "    \"\"\"Récupère tous les textes depuis les fichiers du dossier\"\"\"\n",
    "    corpus_list = []\n",
    "    for file_name in os.listdir(data_dir):\n",
    "        if file_name.endswith(\".txt\"):\n",
    "            file_path = os.path.join(data_dir, file_name)\n",
    "            with open(file_path, \"r\", encoding=\"latin-1\") as f:\n",
    "                lines = [\n",
    "                    line.strip().removeprefix(\"     \")\n",
    "                    for line in f.readlines()\n",
    "                    if line.strip() and not line.startswith(\"<\")\n",
    "                ]\n",
    "                corpus_list.extend(lines)\n",
    "    return corpus_list\n",
    "\n",
    "\n",
    "def save_embedding(text: str, embedding: list, cursor):\n",
    "    \"\"\"Sauvegarde un embedding dans la base PostgreSQL\"\"\"\n",
    "    cursor.execute(\n",
    "        \"INSERT INTO embeddings (corpus, embedding) VALUES (%s, %s)\",\n",
    "        (text, Vector(embedding)) \n",
    "    )\n",
    "\n",
    "\n",
    "def search_pgvector(question: str, top_k: int = 3):\n",
    "    \"\"\"Recherche les textes les plus similaires via pgvector\"\"\"\n",
    "    question_vector = Vector(embed_text(question))  \n",
    "    conn_str = f\"dbname={db_params['dbname']} user={db_params['user']} password={db_params['password']} host={db_params['host']} port={db_params['port']}\"\n",
    "    with psycopg.connect(conn_str) as conn:\n",
    "        register_vector(conn)\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(\"\"\"\n",
    "                SELECT corpus, embedding <-> %s AS distance\n",
    "                FROM embeddings\n",
    "                ORDER BY embedding <-> %s\n",
    "                LIMIT %s\n",
    "            \"\"\", (question_vector, question_vector, top_k))\n",
    "            return cur.fetchall()\n",
    "\n",
    "\n",
    "def llm_answer(question: str, top_k: int = 3):\n",
    "    \"\"\"Génère la réponse en combinant le contexte RAG et le modèle de génération\"\"\"\n",
    "    results = search_pgvector(question, top_k)\n",
    "    if not results:\n",
    "        return \"Je n'ai pas trouvé de réponse pertinente.\"\n",
    "    context = \" \".join([corpus for corpus, _ in results])\n",
    "    prompt = f\"Question: {question}\\nContexte: {context}\\nRéponds clairement :\"\n",
    "    answer = generator(prompt, max_length=200)[0]['generated_text']\n",
    "    return answer\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    conn_str = f\"dbname={db_params['dbname']} user={db_params['user']} password={db_params['password']} host={db_params['host']} port={db_params['port']}\"\n",
    "    try:\n",
    "        with psycopg.connect(conn_str) as conn:\n",
    "            conn.autocommit = True\n",
    "            register_vector(conn)\n",
    "            with conn.cursor() as cur:\n",
    "                cur.execute(\"CREATE EXTENSION IF NOT EXISTS vector;\")\n",
    "                cur.execute(\"\"\"\n",
    "                    CREATE TABLE IF NOT EXISTS embeddings (\n",
    "                        id SERIAL PRIMARY KEY,\n",
    "                        corpus TEXT,\n",
    "                        embedding VECTOR(384)\n",
    "                    )\n",
    "                \"\"\")\n",
    "                cur.execute(\"SELECT COUNT(*) FROM embeddings;\")\n",
    "                if cur.fetchone()[0] == 0:\n",
    "                    print(f\"Lecture des fichiers dans {DATA_DIR}...\")\n",
    "                    corpus_list = create_corpus_list(DATA_DIR)\n",
    "                    print(f\"{len(corpus_list)} textes trouvés, calcul des embeddings...\")\n",
    "                    for i, text in enumerate(corpus_list, 1):\n",
    "                        embedding = embed_text(text)\n",
    "                        save_embedding(text, embedding, cur)\n",
    "                        print(f\"[{i}/{len(corpus_list)}] traité\")\n",
    "                    print(\"Tous les embeddings sont sauvegardés !\")\n",
    "                else:\n",
    "                    print(\"Embeddings déjà présents dans la table.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur PostgreSQL : {e}\")\n",
    "\n",
    "\n",
    "print(\"\\n=== CHATBOT RAG PRÊT ===\")\n",
    "while True:\n",
    "    question = input(\"Votre question : \")\n",
    "    if question.lower() in ['quit', 'exit', 'q']:\n",
    "        print(\"Au revoir !\")\n",
    "        break\n",
    "    if question.strip():\n",
    "        response = llm_answer(question, top_k=3)\n",
    "        print(\"\\nRéponse :\")\n",
    "        print(response)\n",
    "        print(\"=\"*70)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
