{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3cb32729",
      "metadata": {
        "id": "3cb32729",
        "outputId": "040938d3-09b6-4138-b708-0a86208e517c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\hp\\OneDrive\\Bureau\\Chatbot-RAG\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üì• Chargement du mod√®le d'embedding local...\n",
            "‚úÖ Mod√®le charg√© !\n",
            "üöÄ D√©marrage du programme...\n",
            "üìä Cr√©ation de la table...\n",
            "‚úÖ Table cr√©√©e !\n",
            "üìñ Lecture du fichier 023_00000018.txt...\n",
            "‚úÖ 14 textes trouv√©s !\n",
            "‚è≥ Calcul des embeddings localement (rapide)...\n",
            "   [1/14] Traitement...\n",
            "   [2/14] Traitement...\n",
            "   [3/14] Traitement...\n",
            "   [4/14] Traitement...\n",
            "   [5/14] Traitement...\n",
            "   [6/14] Traitement...\n",
            "   [7/14] Traitement...\n",
            "   [8/14] Traitement...\n",
            "   [9/14] Traitement...\n",
            "   [10/14] Traitement...\n",
            "   [11/14] Traitement...\n",
            "   [12/14] Traitement...\n",
            "   [13/14] Traitement...\n",
            "   [14/14] Traitement...\n",
            "‚úÖ Tous les embeddings sont sauvegard√©s !\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import psycopg\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# ===== CONFIGURATION =====\n",
        "conversation_file_path = \"023_00000018.txt\"\n",
        "\n",
        "db_params = {\n",
        "    'dbname': 'rag_chatbot',\n",
        "    'user': 'postgres',\n",
        "    'password': '11649303', \n",
        "    'host': 'localhost',\n",
        "    'port': '5432'\n",
        "}\n",
        "\n",
        "print(\"Chargement du mod√®le d'embedding local\")\n",
        "model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
        "print(\" Mod√®le charg√© !\")\n",
        "\n",
        "def create_conversation_list(file_path: str) -> list[str]:\n",
        "    \"\"\"Lit le fichier texte et retourne une liste de phrases\"\"\"\n",
        "    with open(file_path, \"r\", encoding=\"latin-1\") as file:\n",
        "        text_list = [ligne.strip().removeprefix(\"     \") for ligne in file.readlines() \n",
        "                     if ligne.strip() and not ligne.startswith(\"<\")]\n",
        "    return text_list\n",
        "\n",
        "def calculate_embeddings(corpus: str) -> list[float]:\n",
        "    \"\"\"Calcule l'embedding d'un texte avec le mod√®le local\"\"\"\n",
        "    embedding = model.encode(corpus)\n",
        "    return embedding.tolist()\n",
        "\n",
        "def cosine_similarity(vec1, vec2):\n",
        "    \"\"\"Calcule la similarit√© entre deux vecteurs\"\"\"\n",
        "    vec1 = np.array(vec1)\n",
        "    vec2 = np.array(vec2)\n",
        "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
        "\n",
        "def save_embedding(corpus: str, embedding: list[float], cursor) -> None:\n",
        "    \"\"\"Sauvegarde un texte et son embedding dans la base\"\"\"\n",
        "    cursor.execute(\"INSERT INTO embeddings (corpus, embedding) VALUES (%s, %s)\", \n",
        "                   (corpus, embedding))\n",
        "\n",
        "def similar_corpus(input_corpus: str, top_k: int = 5) -> list[tuple[int, str, float]]:\n",
        "    \"\"\"Trouve les textes les plus similaires √† input_corpus\"\"\"\n",
        "    input_embedding = calculate_embeddings(input_corpus)\n",
        "    \n",
        "    conn_str = f\"dbname={db_params['dbname']} user={db_params['user']} password={db_params['password']} host={db_params['host']} port={db_params['port']}\"\n",
        "    \n",
        "    with psycopg.connect(conn_str) as conn:\n",
        "        with conn.cursor() as cur:\n",
        "            cur.execute(\"SELECT id, corpus, embedding FROM embeddings\")\n",
        "            results = cur.fetchall()\n",
        "            \n",
        "            similarities = []\n",
        "            for id, corpus, embedding in results:\n",
        "                similarity = cosine_similarity(input_embedding, embedding)\n",
        "                similarities.append((id, corpus, similarity))\n",
        "            \n",
        "            similarities.sort(key=lambda x: x[2], reverse=True)\n",
        "            return similarities[:top_k]\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    conn_str = f\"dbname={db_params['dbname']} user={db_params['user']} password={db_params['password']} host={db_params['host']} port={db_params['port']}\"\n",
        "    \n",
        "    try:\n",
        "        print(\" D√©marrage du programme\")\n",
        "        \n",
        "        with psycopg.connect(conn_str) as conn:\n",
        "            conn.autocommit = True\n",
        "            with conn.cursor() as cur:\n",
        "                print(\" Cr√©ation de la table\")\n",
        "                cur.execute(\"DROP TABLE IF EXISTS embeddings\")\n",
        "                cur.execute(\"\"\"\n",
        "                    CREATE TABLE embeddings (\n",
        "                        id SERIAL PRIMARY KEY,\n",
        "                        corpus TEXT,\n",
        "                        embedding FLOAT[]\n",
        "                    )\n",
        "                \"\"\")\n",
        "                print(\"Table cr√©√©e \")\n",
        "                \n",
        "                print(f\"Lecture du fichier {conversation_file_path}\")\n",
        "                corpus_list = create_conversation_list(conversation_file_path)\n",
        "                print(f\" {len(corpus_list)} textes trouv√©s !\")\n",
        "                \n",
        "                print(\" Calcul des embeddings localement (rapide)\")\n",
        "                for i, corpus in enumerate(corpus_list, 1):\n",
        "                    print(f\"   [{i}/{len(corpus_list)}] Traitement\")\n",
        "                    embedding = calculate_embeddings(corpus)\n",
        "                    save_embedding(corpus, embedding, cur)\n",
        "                \n",
        "                conn.commit()\n",
        "                print(\"Tous les embeddings sont sauvegard√©s !\\n\")\n",
        "        \n",
        "    except FileNotFoundError:\n",
        "        print(f\" Erreur : Le fichier '{conversation_file_path}' n'existe pas !\")\n",
        "    except psycopg.OperationalError as e:\n",
        "        print(f\" Erreur de connexion √† PostgreSQL :\")\n",
        "        print(f\"   V√©rifiez que PostgreSQL est d√©marr√©\")\n",
        "        print(f\"   V√©rifiez votre mot de passe\")\n",
        "        print(f\"   D√©tails : {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\" Erreur : {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "968df545",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üì• Chargement du mod√®le de g√©n√©ration (cela peut prendre du temps la premi√®re fois)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "c:\\Users\\hp\\OneDrive\\Bureau\\Chatbot-RAG\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:798: UserWarning: Not enough free disk space to download the file. The expected file size is: 0.79 MB. The target location C:\\Users\\hp\\.cache\\huggingface\\hub\\models--google--flan-t5-base\\blobs only has 0.58 MB free disk space.\n",
            "  warnings.warn(\n",
            "c:\\Users\\hp\\OneDrive\\Bureau\\Chatbot-RAG\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:798: UserWarning: Not enough free disk space to download the file. The expected file size is: 0.79 MB. The target location C:\\Users\\hp\\.cache\\huggingface\\hub\\models--google--flan-t5-base\\blobs only has 0.50 MB free disk space.\n",
            "  warnings.warn(\n",
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "c:\\Users\\hp\\OneDrive\\Bureau\\Chatbot-RAG\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:798: UserWarning: Not enough free disk space to download the file. The expected file size is: 2.42 MB. The target location C:\\Users\\hp\\.cache\\huggingface\\hub\\models--google--flan-t5-base\\blobs only has 0.00 MB free disk space.\n",
            "  warnings.warn(\n",
            "c:\\Users\\hp\\OneDrive\\Bureau\\Chatbot-RAG\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:798: UserWarning: Not enough free disk space to download the file. The expected file size is: 0.00 MB. The target location C:\\Users\\hp\\.cache\\huggingface\\hub\\models--google--flan-t5-base\\blobs only has 0.00 MB free disk space.\n",
            "  warnings.warn(\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Mod√®le de g√©n√©ration charg√© !\n",
            "üéØ CHATBOT RAG\n",
            "\n",
            "\n",
            "üîç Recherche pour : 'qu elle est le numero de telephone'\n",
            "\n",
            "ü§ñ Voici les extraits les plus pertinents :\n",
            "\n",
            "======================================================================\n",
            "\n",
            "1. Pertinence: 80.1%\n",
            "   c: ah ben √©coutez j'ai j'ai e je sais pas j'ai simplement le num√©ro de t√©l√©phone e sur lequel je vous ai appel√©\n",
            "\n",
            "2. Pertinence: 47.3%\n",
            "   c: je e je suis d√©sol√©e j'ai simplement le 0 2 97 0 1 27 25\n",
            "\n",
            "3. Pertinence: 46.3%\n",
            "   h: vous pouvez me dire le bureau ou alors e\n",
            "\n",
            "======================================================================\n",
            "\n",
            "üîç Recherche pour : 'num√©ro de t√©l√©phone'\n",
            "\n",
            "ü§ñ Voici les extraits les plus pertinents :\n",
            "\n",
            "======================================================================\n",
            "\n",
            "1. Pertinence: 75.3%\n",
            "   c: ah ben √©coutez j'ai j'ai e je sais pas j'ai simplement le num√©ro de t√©l√©phone e sur lequel je vous ai appel√©\n",
            "\n",
            "2. Pertinence: 41.9%\n",
            "   c: je e je suis d√©sol√©e j'ai simplement le 0 2 97 0 1 27 25\n",
            "\n",
            "3. Pertinence: 38.6%\n",
            "   c: oui bonjour excusez moi de vous d√©ranger est ce que c'est possible d'avoir Pr√©nom Nom s'il vous pla√Æt\n",
            "\n",
            "======================================================================\n",
            "üëã Au revoir !\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Charger un mod√®le de g√©n√©ration de texte\n",
        "print(\" Chargement du mod√®le de g√©n√©ration \")\n",
        "generator = pipeline('text2text-generation', model='google/flan-t5-base')\n",
        "print(\" Mod√®le de g√©n√©ration charg√© !\")\n",
        "\n",
        "def chatbot_simple(question: str, top_k: int = 3):\n",
        "    \"\"\"Chatbot simple qui affiche les r√©sultats pertinents\"\"\"\n",
        "    \n",
        "    print(f\"\\nRecherche pour : '{question}'\\n\")\n",
        "    results = similar_corpus(question, top_k=top_k)\n",
        "    \n",
        "    print(\" Voici les extraits les plus pertinents :\\n\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    for i, (id, corpus, score) in enumerate(results, 1):\n",
        "        print(f\"\\n{i}. Pertinence: {score:.1%}\")\n",
        "        print(f\"   {corpus}\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "\n",
        "# TEST\n",
        "print(\"CHATBOT RAG\\n\")\n",
        "\n",
        "while True:\n",
        "    question = input(\" Votre question : \")\n",
        "    \n",
        "    if question.lower() in ['quit', 'exit', 'q']:\n",
        "        print(\" Au revoir !\")\n",
        "        break\n",
        "    \n",
        "    if question.strip():\n",
        "        chatbot_simple(question, top_k=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65764868",
      "metadata": {
        "id": "65764868"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
