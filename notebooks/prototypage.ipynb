{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "65764868",
      "metadata": {
        "id": "65764868"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd585256",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chargement du modèle d'embedding...\n",
            "Modèle d'embedding chargé !\n",
            "Chargement du modèle de génération...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Modèle de génération chargé !\n",
            "Embeddings déjà présents dans la table.\n",
            "\n",
            "=== CHATBOT RAG PRÊT ===\n"
          ]
        },
        {
          "ename": "UndefinedFunction",
          "evalue": "l'opérateur n'existe pas : double precision[] <-> vector\nLINE 2:                 SELECT corpus, embedding <-> $1 AS distance\n                                                 ^\nHINT:  Aucun opérateur ne correspond au nom donné et aux types d'arguments.\nVous devez ajouter des conversions explicites de type.",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mUndefinedFunction\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 147\u001b[39m\n\u001b[32m    145\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m question.strip():\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     response = \u001b[43mllm_answer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    148\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mRéponse :\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    149\u001b[39m     \u001b[38;5;28mprint\u001b[39m(response)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 90\u001b[39m, in \u001b[36mllm_answer\u001b[39m\u001b[34m(question, top_k)\u001b[39m\n\u001b[32m     88\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mllm_answer\u001b[39m(question: \u001b[38;5;28mstr\u001b[39m, top_k: \u001b[38;5;28mint\u001b[39m = \u001b[32m3\u001b[39m):\n\u001b[32m     89\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Génère la réponse en combinant le contexte RAG et le modèle de génération\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m     results = \u001b[43msearch_pgvector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     91\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m results:\n\u001b[32m     92\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mJe n\u001b[39m\u001b[33m'\u001b[39m\u001b[33mai pas trouvé de réponse pertinente.\u001b[39m\u001b[33m\"\u001b[39m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 79\u001b[39m, in \u001b[36msearch_pgvector\u001b[39m\u001b[34m(question, top_k)\u001b[39m\n\u001b[32m     77\u001b[39m register_vector(conn)\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m conn.cursor() \u001b[38;5;28;01mas\u001b[39;00m cur:\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m     \u001b[43mcur\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\"\"\u001b[39;49m\n\u001b[32m     80\u001b[39m \u001b[33;43m        SELECT corpus, embedding <-> \u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[33;43m AS distance\u001b[39;49m\n\u001b[32m     81\u001b[39m \u001b[33;43m        FROM embeddings\u001b[39;49m\n\u001b[32m     82\u001b[39m \u001b[33;43m        ORDER BY embedding <-> \u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\n\u001b[32m     83\u001b[39m \u001b[33;43m        LIMIT \u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\n\u001b[32m     84\u001b[39m \u001b[33;43m    \u001b[39;49m\u001b[33;43m\"\"\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion_vector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion_vector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     85\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cur.fetchall()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\OneDrive\\Bureau\\Chatbot-RAG\\venv\\Lib\\site-packages\\psycopg\\cursor.py:97\u001b[39m, in \u001b[36mCursor.execute\u001b[39m\u001b[34m(self, query, params, prepare, binary)\u001b[39m\n\u001b[32m     93\u001b[39m         \u001b[38;5;28mself\u001b[39m._conn.wait(\n\u001b[32m     94\u001b[39m             \u001b[38;5;28mself\u001b[39m._execute_gen(query, params, prepare=prepare, binary=binary)\n\u001b[32m     95\u001b[39m         )\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m e._NO_TRACEBACK \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ex.with_traceback(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
            "\u001b[31mUndefinedFunction\u001b[39m: l'opérateur n'existe pas : double precision[] <-> vector\nLINE 2:                 SELECT corpus, embedding <-> $1 AS distance\n                                                 ^\nHINT:  Aucun opérateur ne correspond au nom donné et aux types d'arguments.\nVous devez ajouter des conversions explicites de type."
          ]
        }
      ],
      "source": [
        "\n",
        "import os\n",
        "import psycopg\n",
        "from pgvector import Vector\n",
        "from pgvector.psycopg import register_vector\n",
        "from transformers import pipeline, AutoTokenizer, AutoModel\n",
        "import torch\n",
        "\n",
        "DATA_DIR = \"data/TRANS_TXT\" \n",
        "db_params = {\n",
        "    'dbname': 'rag_chatbot',\n",
        "    'user': 'postgres',\n",
        "    'password': '11649303',\n",
        "    'host': 'localhost',\n",
        "    'port': '5432'\n",
        "}\n",
        "VECTOR_DIM = 384  \n",
        "\n",
        "\n",
        "print(\"Chargement du modèle d'embedding...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "embedding_model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "print(\"Modèle d'embedding chargé !\")\n",
        "\n",
        "print(\"Chargement du modèle de génération...\")\n",
        "generator = pipeline('text2text-generation', model='google/flan-t5-base')\n",
        "print(\"Modèle de génération chargé !\")\n",
        "\n",
        "\n",
        "\n",
        "def embed_text(text: str):\n",
        "    \"\"\"Calcule l'embedding d'un texte via Hugging Face et renvoie une liste\"\"\"\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "    with torch.no_grad():\n",
        "        embeddings = embedding_model(**inputs).last_hidden_state.mean(dim=1)\n",
        "    return embeddings.squeeze().tolist()\n",
        "\n",
        "\n",
        "def create_corpus_list(data_dir: str):\n",
        "    \"\"\"Récupère tous les textes depuis les fichiers du dossier\"\"\"\n",
        "    corpus_list = []\n",
        "    for file_name in os.listdir(data_dir):\n",
        "        if file_name.endswith(\".txt\"):\n",
        "            file_path = os.path.join(data_dir, file_name)\n",
        "            with open(file_path, \"r\", encoding=\"latin-1\") as f:\n",
        "                lines = [\n",
        "                    line.strip().removeprefix(\"     \")\n",
        "                    for line in f.readlines()\n",
        "                    if line.strip() and not line.startswith(\"<\")\n",
        "                ]\n",
        "                corpus_list.extend(lines)\n",
        "    return corpus_list\n",
        "\n",
        "\n",
        "def save_embedding(text: str, embedding: list, cursor):\n",
        "    \"\"\"Sauvegarde un embedding dans la base PostgreSQL\"\"\"\n",
        "    cursor.execute(\n",
        "        \"INSERT INTO embeddings (corpus, embedding) VALUES (%s, %s)\",\n",
        "        (text, Vector(embedding)) \n",
        "    )\n",
        "\n",
        "\n",
        "def search_pgvector(question: str, top_k: int = 3):\n",
        "    \"\"\"Recherche les textes les plus similaires via pgvector\"\"\"\n",
        "    question_vector = Vector(embed_text(question))  \n",
        "    conn_str = f\"dbname={db_params['dbname']} user={db_params['user']} password={db_params['password']} host={db_params['host']} port={db_params['port']}\"\n",
        "    with psycopg.connect(conn_str) as conn:\n",
        "        register_vector(conn)\n",
        "        with conn.cursor() as cur:\n",
        "            cur.execute(\"\"\"\n",
        "                SELECT corpus, embedding <-> %s AS distance\n",
        "                FROM embeddings\n",
        "                ORDER BY embedding <-> %s\n",
        "                LIMIT %s\n",
        "            \"\"\", (question_vector, question_vector, top_k))\n",
        "            return cur.fetchall()\n",
        "\n",
        "\n",
        "def llm_answer(question: str, top_k: int = 3):\n",
        "    \"\"\"Génère la réponse en combinant le contexte RAG et le modèle de génération\"\"\"\n",
        "    results = search_pgvector(question, top_k)\n",
        "    if not results:\n",
        "        return \"Je n'ai pas trouvé de réponse pertinente.\"\n",
        "    context = \" \".join([corpus for corpus, _ in results])\n",
        "    prompt = f\"Question: {question}\\nContexte: {context}\\nRéponds clairement :\"\n",
        "    answer = generator(prompt, max_length=200)[0]['generated_text']\n",
        "    return answer\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    conn_str = f\"dbname={db_params['dbname']} user={db_params['user']} password={db_params['password']} host={db_params['host']} port={db_params['port']}\"\n",
        "    try:\n",
        "        with psycopg.connect(conn_str) as conn:\n",
        "            conn.autocommit = True\n",
        "            register_vector(conn)\n",
        "            with conn.cursor() as cur:\n",
        "                cur.execute(\"CREATE EXTENSION IF NOT EXISTS vector;\")\n",
        "                cur.execute(\"\"\"\n",
        "                    CREATE TABLE IF NOT EXISTS embeddings (\n",
        "                        id SERIAL PRIMARY KEY,\n",
        "                        corpus TEXT,\n",
        "                        embedding VECTOR(384)\n",
        "                    )\n",
        "                \"\"\")\n",
        "                cur.execute(\"SELECT COUNT(*) FROM embeddings;\")\n",
        "                if cur.fetchone()[0] == 0:\n",
        "                    print(f\"Lecture des fichiers dans {DATA_DIR}...\")\n",
        "                    corpus_list = create_corpus_list(DATA_DIR)\n",
        "                    print(f\"{len(corpus_list)} textes trouvés, calcul des embeddings...\")\n",
        "                    for i, text in enumerate(corpus_list, 1):\n",
        "                        embedding = embed_text(text)\n",
        "                        save_embedding(text, embedding, cur)\n",
        "                        print(f\"[{i}/{len(corpus_list)}] traité\")\n",
        "                    print(\"Tous les embeddings sont sauvegardés !\")\n",
        "                else:\n",
        "                    print(\"Embeddings déjà présents dans la table.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Erreur PostgreSQL : {e}\")\n",
        "\n",
        "\n",
        "print(\"\\n=== CHATBOT RAG PRÊT ===\")\n",
        "while True:\n",
        "    question = input(\"Votre question : \")\n",
        "    if question.lower() in ['quit', 'exit', 'q']:\n",
        "        print(\"Au revoir !\")\n",
        "        break\n",
        "    if question.strip():\n",
        "        response = llm_answer(question, top_k=3)\n",
        "        print(\"\\nRéponse :\")\n",
        "        print(response)\n",
        "        print(\"=\"*70)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
